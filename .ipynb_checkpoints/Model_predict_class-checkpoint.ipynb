{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import torch\n",
    "import time\n",
    "import random\n",
    "from torch.nn import Linear, Sigmoid, ReLU, Dropout, MSELoss, Sequential, Module, BatchNorm1d, Dropout\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from torch.optim import Adam, SGD\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class for the neural network model\n",
    "class Net(Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.drop_prob = 0.2\n",
    "        self.drop = Dropout(self.drop_prob)\n",
    "\n",
    "        #The linear fully connected layer at the end of the network that outputs a classification\n",
    "        self.out_layers = Sequential(\n",
    "            BatchNorm1d(100),    \n",
    "            Linear(100, 256),\n",
    "            BatchNorm1d(256),\n",
    "            ReLU(),\n",
    "            Linear(256, 512),\n",
    "            BatchNorm1d(512),\n",
    "            ReLU(),\n",
    "            Linear(512, 1024),\n",
    "            BatchNorm1d(1024),\n",
    "            self.drop,\n",
    "            ReLU(),\n",
    "            Linear(1024, 1024),\n",
    "            #BatchNorm1d(1024),\n",
    "            ReLU(),\n",
    "            Linear(1024, 1024),\n",
    "            self.drop,\n",
    "            BatchNorm1d(1024),\n",
    "            ReLU(),\n",
    "            Linear(1024, 512),\n",
    "            BatchNorm1d(512),\n",
    "            ReLU(),\n",
    "            Linear(512, 256),\n",
    "            ReLU(),\n",
    "            Linear(256, 256),\n",
    "            ReLU(),\n",
    "            Linear(256, 256),\n",
    "            BatchNorm1d(256),\n",
    "            ReLU(),\n",
    "            Linear(256, 256),\n",
    "            BatchNorm1d(256),\n",
    "            ReLU(),\n",
    "            Linear(256, 64),\n",
    "            BatchNorm1d(64),\n",
    "            ReLU(),\n",
    "            Linear(64, 64),\n",
    "            BatchNorm1d(64),\n",
    "            ReLU(),\n",
    "            Linear(64, 16),            \n",
    "            BatchNorm1d(16),\n",
    "            ReLU(),\n",
    "            Linear(16, 1),\n",
    "            Sigmoid()\n",
    "        )\n",
    "\n",
    "\n",
    "    #Define the forward pass through the model \n",
    "    def forward(self, x):\n",
    "        x = torch.sum(x, 1)\n",
    "        x = x.reshape((x.shape[0],x.shape[1]))\n",
    "        x = self.out_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NegationCheck(object):\n",
    "    \n",
    "    \n",
    "    #Initialise the model and the fasttext object from the path to model and \n",
    "    #the embeddings file for the fasttext object\n",
    "    def __init__(self, path_to_model):\n",
    "        self.model = Net()\n",
    "        self.model.load_state_dict(torch.load(path_to_model))\n",
    "        self.model = self.model.float()\n",
    "        self.model.eval()\n",
    "        self.ft = fasttext.load_model(\"cc.tr.100.bin\")\n",
    "        self.optimizer = Adam(self.model.parameters(), lr=0.001)\n",
    "        self.loss = MSELoss()\n",
    "    \n",
    "    def load_model(self, path_to_model):\n",
    "        self.model.load_state_dict(torch.load(path_to_model))\n",
    "        return\n",
    "    \n",
    "    #Given the input string, returns the vectorized representations obtained from the embeddings of the fasttext model\n",
    "    #sentences: a batch of strings that are the inputs to the model\n",
    "    #in_vector: [batch_size, num_inp_words, 100, 1] Tensor - reshape the vectors to \n",
    "    #           (100 x 1) matrices and stack them as channels\n",
    "    def input_vectors(self, sentences: list):\n",
    "\n",
    "        max_num_words = 4\n",
    "        tokens = []\n",
    "        batch_size = len(sentences)\n",
    "        \n",
    "        h, w = (100,1)\n",
    "\n",
    "        #loop over the batches to tokenize the inputs\n",
    "        for i in range(batch_size):\n",
    "            #Tokenize words using default fasttext tokenizer, which creates tokens \n",
    "            # by dividing splitting at word separating chars\n",
    "            tokens.append(fasttext.tokenize(sentences[i]))\n",
    "\n",
    "        #Create a matrix with batch_size batches, num token channels and 100x1 matrices to store the 100dim embeddings\n",
    "        in_vector = np.zeros((batch_size, max_num_words, h, w))\n",
    "\n",
    "\n",
    "        #cycle over the tokens and get their vectors, reshape them to 100x1 and store in the corresponding \n",
    "        #channel in the return variable\n",
    "        \n",
    "        #cycle over the entire batch\n",
    "        for j in range(len(tokens)):\n",
    "\n",
    "            #counter for tokens\n",
    "            i = 0 \n",
    "            \n",
    "            #cycle over tokens\n",
    "            for token in tokens[j]:\n",
    "                \n",
    "                #get the embedding for the single token\n",
    "                vector = torch.tensor(self.ft[token].astype(np.double))\n",
    "                \n",
    "                #reshape it to desired dims\n",
    "                vector = vector.reshape(h,w)\n",
    "                \n",
    "                #Store it in the input vectors matrix\n",
    "                in_vector[j][i] = vector\n",
    "                \n",
    "                #increment the position of the word index within the given sentence\n",
    "                #if it goes over the max word size, cut\n",
    "                i=i+1\n",
    "                if(i == max_num_words):\n",
    "                    break\n",
    "\n",
    "        #create a tensor object to return\n",
    "        in_vector = torch.tensor(in_vector)\n",
    "\n",
    "        return in_vector\n",
    "\n",
    "    #Given a list of Turkish sentences, returns a list of True/False for their negativity\n",
    "    #sentences: a list of sentence strings\n",
    "    def is_negative(self, sentences):\n",
    "        \n",
    "        #get word vectors from the fasttext embeddings as torch.tensor\n",
    "        sentence_vectors = self.input_vectors(sentences)\n",
    "        \n",
    "        #get the predictions from the model for the sentences\n",
    "        outputs = self.model(sentence_vectors.float()).detach()\n",
    "        \n",
    "        #reshape the output from (n,1) to (n)\n",
    "        outputs = outputs.reshape(outputs.shape[0])\n",
    "        \n",
    "        #get the output values as confidence scores\n",
    "        confidences = outputs.clone().detach()\n",
    "        \n",
    "        #turn the tensor objects into numpy arrays\n",
    "        confidences = confidences.numpy()\n",
    "        outputs = outputs.numpy()\n",
    "        \n",
    "        #if the prediction is that the sentence is positive, then fix the confidence as 1-c\n",
    "        for i in range(len(confidences)):\n",
    "            if confidences[i] < 0.5:\n",
    "                confidences[i] = 1 - confidences[i] \n",
    "        \n",
    "        #turn the outputs into True/False values\n",
    "        outputs = outputs > 0.5\n",
    "        \n",
    "        #return lists of outputs and confidences\n",
    "        return outputs, confidences\n",
    "    \n",
    "    \n",
    "    \n",
    "    def data_from_txt(self, file_path):\n",
    "\n",
    "        file = open(file_path, 'r')\n",
    "\n",
    "        data_x = []\n",
    "        data_y = []\n",
    "\n",
    "        n = 0\n",
    "        newline = '\\n'\n",
    "                   \n",
    "        #Loop through the lines, and split at the comma for the inputs x and labels y\n",
    "        for line in file:\n",
    "\n",
    "            temp = line.split(\",\")\n",
    "            \n",
    "            temp[1] = temp[1].strip()\n",
    "            \n",
    "            if temp[0] == '':\n",
    "                break\n",
    "\n",
    "            data_y.append(float(temp[0]))\n",
    "            \n",
    "            if temp[1].endswith(newline):\n",
    "                temp[1].replace(newline, '')\n",
    "                   \n",
    "            data_x.append(temp[1])\n",
    "\n",
    "            n = n + 1\n",
    "\n",
    "        file.close()\n",
    "\n",
    "        return (data_x, data_y, n)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def count_parameters(self):\n",
    "        return sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def print_model(self):\n",
    "        print(self.model)\n",
    "        print(\"Total Parameters: \", count_parameters(self.model))\n",
    "        return\n",
    "    \n",
    "    \n",
    "    def make_dataset(self, X, Y, n, verbose):\n",
    "        \n",
    "        X = self.input_vectors(X)\n",
    "        Y = torch.tensor(Y)\n",
    "        Y = Y.reshape(n, 1)\n",
    "        \n",
    "        if verbose:\n",
    "            print(Y.shape)\n",
    "            print(X.shape)\n",
    "        \n",
    "        dataset = TensorDataset(X, Y)\n",
    "\n",
    "        return dataset\n",
    "    \n",
    "    \n",
    "    def train(self, train_data, test_data, epoch, lr=0.001, verbose=False):\n",
    "        \n",
    "        rand_seed = 40\n",
    "        torch.manual_seed(rand_seed)\n",
    "        np.random.seed(rand_seed)\n",
    "        random.seed(rand_seed)\n",
    "\n",
    "        self.optimizer = Adam(self.model.parameters(), lr)\n",
    "        \n",
    "        X, Y, n = train_data\n",
    "        X_test, Y_test, n_test = test_data\n",
    "\n",
    "        data_train = self.make_dataset(X, Y, n, verbose)\n",
    "        \n",
    "        data_test = self.make_dataset(X_test, Y_test, n_test, verbose)\n",
    "        \n",
    "        train_loader = DataLoader(data_train, batch_size=20, shuffle = True)\n",
    "        test_loader = DataLoader(data_test, batch_size=n_test, shuffle=False)\n",
    "\n",
    "        self.model = self.model.float()\n",
    "        \n",
    "        if verbose:\n",
    "            print_model()\n",
    "        \n",
    "        train_losses = []\n",
    "        train_weights = []\n",
    "        test_accs = []\n",
    "        test_losses = []\n",
    "\n",
    "        for e in range(epoch):\n",
    "\n",
    "            train_loss, weight = self.train_epoch(train_loader)\n",
    "            test_acc, test_loss = self.test_epoch(test_loader)\n",
    "                \n",
    "            train_losses.append(train_loss)\n",
    "            test_accs.append(test_acc)\n",
    "            test_losses.append(test_loss)\n",
    "            train_weights.append(weight)\n",
    "            \n",
    "            \n",
    "            print(\"Epoch: \", e+1, \"\\tTrain Loss: %.3f\" %train_loss.item(), \n",
    "                  \"\\tTest Loss: %.3f\" %test_loss, \"\\tTest Acc: %.2f\" %test_acc)\n",
    "\n",
    "            \n",
    "        plt.plot(train_losses, label=\"Training Losses\")\n",
    "        plt.plot(test_losses, label=\"Test Losses\")\n",
    "        plt.legend()\n",
    "        plt.show() \n",
    "        plt.plot(test_accs, label=\"Test Accuracy\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "        self.model.eval()\n",
    "        \n",
    "        return\n",
    "    \n",
    "    \n",
    "    def train_epoch(self, train_loader):\n",
    "        \n",
    "        self.model.train()\n",
    "    \n",
    "        last_loss = 0\n",
    "\n",
    "        for i, data in enumerate(train_loader, 0):    \n",
    "\n",
    "            inputs, labels = data\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "                        \n",
    "            X_out = self.model(inputs.float())\n",
    "\n",
    "            loss_tr = self.loss(X_out, labels)\n",
    "\n",
    "            loss_tr.backward()\n",
    "\n",
    "            self.optimizer.step()\n",
    "\n",
    "            last_loss = loss_tr\n",
    "\n",
    "        s = torch.sum(self.model.out_layers[7].weight.data)\n",
    "\n",
    "        return last_loss, s\n",
    "    \n",
    "\n",
    "    def test_epoch(self, test_loader):\n",
    "        \n",
    "        self.model.eval()\n",
    "    \n",
    "        correct = 0\n",
    "        total = 0\n",
    "        test_loss = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data in test_loader:\n",
    "\n",
    "                sents, labels = data\n",
    "\n",
    "                outputs = self.model(sents.float())\n",
    "\n",
    "                test_loss = self.loss(outputs, labels)\n",
    "\n",
    "                outputs = outputs>=0.5\n",
    "\n",
    "                total += labels.size(0)\n",
    "                correct += (outputs==labels).sum().item()\n",
    "\n",
    "        accuracy = 100*correct/total\n",
    "\n",
    "        return accuracy, test_loss\n",
    "    \n",
    "    \n",
    "    def test_acc(self, test_x, test_y, n):\n",
    "    \n",
    "        inputs = self.input_vectors(test_x)\n",
    "        outputs = self.model(inputs.float())\n",
    "        labels = torch.tensor(test_y)\n",
    "        \n",
    "        correct = 0\n",
    "        total = n\n",
    "        \n",
    "        outputs = outputs.reshape(outputs.shape[0])\n",
    "        \n",
    "        predicts = outputs>=0.5\n",
    "        \n",
    "        correct = (predicts == labels)\n",
    "        \n",
    "        num_correct = correct.sum()\n",
    "        \n",
    "        accuracy = 100*num_correct/total\n",
    "\n",
    "        print(\"Accuracy on the validation set of \", total, \" items is: \", accuracy.item())\n",
    "\n",
    "        return correct, predicts, outputs\n",
    "    \n",
    "    \n",
    "    def test_validate(self, path_to_file):\n",
    "        \n",
    "        x, y, n = self.data_from_txt(path_to_file)\n",
    "        correct, preds, outs = self.test_acc(x, y, n)\n",
    "\n",
    "        labels = torch.tensor(y)\n",
    "        false_ind = [i for i in range(len(x)) if correct[i]==False]\n",
    "        false_sents = [x[i] for i in range(len(x)) if correct[i]==False]\n",
    "        false_preds = [int(preds[i]) for i in range(len(x)) if correct[i]==False]\n",
    "        labs = [int(y[i]) for i in range(len(x)) if correct[i]==False]\n",
    "        f_outs = [float(outs[i]) for i in range(len(x)) if correct[i]==False]\n",
    "        falses = [false_ind, false_sents, labs, false_preds, f_outs]\n",
    "        falses = list(map(list, zip(*falses)))\n",
    "\n",
    "        for i in range(len(false_ind)):\n",
    "            print(\"Ind: \", falses[i][0], \"\\tSent: \", falses[i][1], \"Label: \", \n",
    "                  falses[i][2], \"Model Pred: %d \" %falses[i][3], \"%0.2f\" %falses[i][4])\n",
    "        print(false_sents)\n",
    "        \n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "nc = NegationCheck(\"./model_state.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([2, 4, 100, 1])\n",
      "Epoch:  1 \tTrain Loss: 0.426 \tTest Loss: 0.205 \tTest Acc: 74.00\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([2, 4, 100, 1])\n",
      "Epoch:  2 \tTrain Loss: 0.008 \tTest Loss: 0.176 \tTest Acc: 77.00\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([2, 4, 100, 1])\n",
      "Epoch:  3 \tTrain Loss: 0.008 \tTest Loss: 0.124 \tTest Acc: 82.00\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([2, 4, 100, 1])\n",
      "Epoch:  4 \tTrain Loss: 0.008 \tTest Loss: 0.144 \tTest Acc: 81.00\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([2, 4, 100, 1])\n",
      "Epoch:  5 \tTrain Loss: 0.431 \tTest Loss: 0.154 \tTest Acc: 81.00\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([2, 4, 100, 1])\n",
      "Epoch:  6 \tTrain Loss: 0.420 \tTest Loss: 0.172 \tTest Acc: 76.00\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([2, 4, 100, 1])\n",
      "Epoch:  7 \tTrain Loss: 0.007 \tTest Loss: 0.160 \tTest Acc: 75.00\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([2, 4, 100, 1])\n",
      "Epoch:  8 \tTrain Loss: 0.006 \tTest Loss: 0.154 \tTest Acc: 76.00\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([2, 4, 100, 1])\n",
      "Epoch:  9 \tTrain Loss: 0.005 \tTest Loss: 0.186 \tTest Acc: 72.00\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([2, 4, 100, 1])\n",
      "Epoch:  10 \tTrain Loss: 0.863 \tTest Loss: 0.158 \tTest Acc: 80.00\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([2, 4, 100, 1])\n",
      "Epoch:  11 \tTrain Loss: 0.856 \tTest Loss: 0.143 \tTest Acc: 81.00\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([2, 4, 100, 1])\n",
      "Epoch:  12 \tTrain Loss: 0.006 \tTest Loss: 0.168 \tTest Acc: 77.00\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([2, 4, 100, 1])\n",
      "Epoch:  13 \tTrain Loss: 0.006 \tTest Loss: 0.191 \tTest Acc: 73.00\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([2, 4, 100, 1])\n",
      "Epoch:  14 \tTrain Loss: 0.417 \tTest Loss: 0.165 \tTest Acc: 78.00\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([2, 4, 100, 1])\n",
      "Epoch:  15 \tTrain Loss: 0.413 \tTest Loss: 0.171 \tTest Acc: 76.00\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([2, 4, 100, 1])\n",
      "Epoch:  16 \tTrain Loss: 0.838 \tTest Loss: 0.157 \tTest Acc: 76.00\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([2, 4, 100, 1])\n",
      "Epoch:  17 \tTrain Loss: 0.008 \tTest Loss: 0.159 \tTest Acc: 75.00\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n",
      "torch.Size([20, 4, 100, 1])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-228-1932e2cd246a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_from_txt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./train.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_from_txt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./test.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m45\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-225-51ae22ea285d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_data, test_data, epoch, lr, verbose)\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m             \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m             \u001b[0mtest_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-225-51ae22ea285d>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(self, train_loader)\u001b[0m\n\u001b[1;32m    242\u001b[0m             \u001b[0mloss_tr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m             \u001b[0mlast_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_tr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'betas'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m             F.adam(params_with_grad,\n\u001b[0m\u001b[1;32m    109\u001b[0m                    \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m                    \u001b[0mexp_avgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/optim/functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nc.train(nc.data_from_txt(\"./train.txt\"), nc.data_from_txt(\"./test.txt\"), epoch=45, lr=0.001, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the validation set of  100  items is:  67.0\n",
      "Ind:  1 \tSent:  yedi oda bizi aşar Label:  1 Model Pred: 0  0.10\n",
      "Ind:  2 \tSent:  beşiktaş uzak kalır Label:  1 Model Pred: 0  0.09\n",
      "Ind:  4 \tSent:  bize uygun Label:  0 Model Pred: 1  0.90\n",
      "Ind:  5 \tSent:  çok pahalı Label:  1 Model Pred: 0  0.11\n",
      "Ind:  7 \tSent:  iki oda az Label:  1 Model Pred: 0  0.11\n",
      "Ind:  9 \tSent:  manzarası yoksa olmaz Label:  1 Model Pred: 0  0.12\n",
      "Ind:  12 \tSent:  altı olsun Label:  0 Model Pred: 1  0.90\n",
      "Ind:  13 \tSent:  kredili Label:  0 Model Pred: 1  0.90\n",
      "Ind:  16 \tSent:  bu daireyi kaçırmayalım Label:  0 Model Pred: 1  0.85\n",
      "Ind:  17 \tSent:  kredi işi kolay Label:  0 Model Pred: 1  0.86\n",
      "Ind:  24 \tSent:  toplu taşımaya uzak Label:  1 Model Pred: 0  0.09\n",
      "Ind:  25 \tSent:  ev sahibi geldi Label:  0 Model Pred: 1  0.76\n",
      "Ind:  27 \tSent:  kiracı bulmak kolay Label:  0 Model Pred: 1  0.51\n",
      "Ind:  28 \tSent:  teslim süreci yavaş Label:  1 Model Pred: 0  0.10\n",
      "Ind:  30 \tSent:  o bölge uzak kalır Label:  1 Model Pred: 0  0.09\n",
      "Ind:  35 \tSent:  depreme karşı zayıf Label:  1 Model Pred: 0  0.10\n",
      "Ind:  38 \tSent:  yatırım getirisi az Label:  1 Model Pred: 0  0.40\n",
      "Ind:  41 \tSent:  güney cephesine bakmıyor Label:  1 Model Pred: 0  0.10\n",
      "Ind:  42 \tSent:  diğeri kuzey cephesine bakıyor Label:  1 Model Pred: 0  0.07\n",
      "Ind:  47 \tSent:  iki oda olmaz Label:  1 Model Pred: 0  0.16\n",
      "Ind:  48 \tSent:  yüksek olmazsa daha iyi Label:  1 Model Pred: 0  0.11\n",
      "Ind:  49 \tSent:  yüksek kat olsun Label:  0 Model Pred: 1  0.52\n",
      "Ind:  53 \tSent:  orası dün satıldı Label:  1 Model Pred: 0  0.09\n",
      "Ind:  56 \tSent:  ne yazık ki yok Label:  1 Model Pred: 0  0.09\n",
      "Ind:  57 \tSent:  açık havuz kullanıma kapalı Label:  1 Model Pred: 0  0.09\n",
      "Ind:  71 \tSent:  mutlaka havuzu olsun Label:  0 Model Pred: 1  0.88\n",
      "Ind:  74 \tSent:  metrekaresi yeterli Label:  0 Model Pred: 1  0.89\n",
      "Ind:  83 \tSent:  hemen gelin Label:  0 Model Pred: 1  0.88\n",
      "Ind:  85 \tSent:  tapulu gayrimenkuldur Label:  0 Model Pred: 1  0.91\n",
      "Ind:  89 \tSent:  rezidans arıyorum Label:  0 Model Pred: 1  0.90\n",
      "Ind:  92 \tSent:  üç güne hazır Label:  0 Model Pred: 1  0.89\n",
      "Ind:  93 \tSent:  ben sorumlu değilim Label:  1 Model Pred: 0  0.09\n",
      "Ind:  98 \tSent:  farbika arsası olsun Label:  0 Model Pred: 1  0.91\n",
      "['yedi oda bizi aşar', 'beşiktaş uzak kalır', 'bize uygun', 'çok pahalı', 'iki oda az', 'manzarası yoksa olmaz', 'altı olsun', 'kredili', 'bu daireyi kaçırmayalım', 'kredi işi kolay', 'toplu taşımaya uzak', 'ev sahibi geldi', 'kiracı bulmak kolay', 'teslim süreci yavaş', 'o bölge uzak kalır', 'depreme karşı zayıf', 'yatırım getirisi az', 'güney cephesine bakmıyor', 'diğeri kuzey cephesine bakıyor', 'iki oda olmaz', 'yüksek olmazsa daha iyi', 'yüksek kat olsun', 'orası dün satıldı', 'ne yazık ki yok', 'açık havuz kullanıma kapalı', 'mutlaka havuzu olsun', 'metrekaresi yeterli', 'hemen gelin', 'tapulu gayrimenkuldur', 'rezidans arıyorum', 'üç güne hazır', 'ben sorumlu değilim', 'farbika arsası olsun']\n"
     ]
    }
   ],
   "source": [
    "nc.test_validate(\"./test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out:  False Conf:  0.89222777\n",
      "Out:  True Conf:  0.8994698\n"
     ]
    }
   ],
   "source": [
    "sents = [\"merhaba benim adim alicanhas\", \"olmaz\"]\n",
    "nc.input_vectors(sents)\n",
    "outs, confs = nc.is_negative(sents)\n",
    "for i in range(len(outs)):\n",
    "    print(\"Out: \", outs[i], \"Conf: \", confs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "321\n"
     ]
    }
   ],
   "source": [
    "_,_,n = nc.data_from_txt(\"./train.txt\")\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
