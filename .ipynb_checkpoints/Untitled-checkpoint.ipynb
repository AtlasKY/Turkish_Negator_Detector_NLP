{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import fasttext.util\n",
    "import time\n",
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import Linear, Sigmoid, ReLU, Dropout, Embedding, MSELoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm1d, BatchNorm2d, Dropout\n",
    "from torch.optim import Adam, SGD\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "ft = fasttext.load_model(\"cc.tr.100.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<fasttext.FastText._FastText at 0x7fc8ea166dc0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasttext.util.reduce_model(ft, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft.save_model('cc.tr.100.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft.get_dimension()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.1749, -0.1436, -0.6654, -0.1926, -0.3531, -0.0698, -0.0701, -0.2173,\n",
       "         0.4043, -0.0094, -0.0227,  0.2733,  0.0446, -0.0148, -0.1951,  0.2377,\n",
       "         0.1824,  0.2891, -0.2884, -0.0603,  0.2166,  0.2423, -0.0496, -0.1195,\n",
       "        -0.1218, -0.3270,  0.1299, -0.1416,  0.2645, -0.0136, -0.0608,  0.0016,\n",
       "         0.4760,  0.0261, -0.0905, -0.0853, -0.2786,  0.1290,  0.0092, -0.2808,\n",
       "         0.0391, -0.0272, -0.1067,  0.2834, -0.2156,  0.0348, -0.0155,  0.1579,\n",
       "        -0.0482, -0.0610, -0.0649,  0.2014,  0.0933,  0.0381,  0.0060,  0.0237,\n",
       "         0.1157, -0.0642,  0.0217, -0.0306,  0.0130, -0.0622,  0.0378,  0.1764,\n",
       "         0.0881, -0.1186,  0.0831,  0.0966, -0.0085, -0.0857,  0.2434, -0.1090,\n",
       "         0.1817,  0.0233, -0.0803,  0.0203, -0.0254,  0.1282,  0.0634,  0.0522,\n",
       "        -0.0733, -0.0209, -0.0849,  0.0318,  0.0038, -0.0284,  0.2016, -0.0319,\n",
       "         0.1466, -0.1054,  0.1788, -0.0199, -0.0951, -0.1183,  0.0230,  0.0398,\n",
       "         0.0410, -0.0598,  0.1671, -0.0320])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors = torch.from_numpy(ft.get_input_matrix())\n",
    "word_vectors[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['anamurlulaştırabilemediklerimizdekilerden', 'misiniz']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = ft.words\n",
    "#words\n",
    "#print(ft[words[1]])\n",
    "isp = ft[\"ispanyol\"]\n",
    "las = ft[\"laştıramadıklarımız\"]\n",
    "#print(np.add(isp,las))\n",
    "#print(word_dict[\"anamurlulaştırabilemediklerimizdekilerden\"])\n",
    "fasttext.tokenize('anamurlulaştırabilemediklerimizdekilerden misiniz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = {words[i]: ft[words[i]] for i in range(len(words)) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0694279670715332\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "for i in range(130000):\n",
    "    n = words[int(np.random.rand()*40000)]\n",
    "    ft[n]\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['<pany', 'pany>'], array([2919705, 2308259]))"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft.get_subwords('ispanyollaştıramadıklarımızdan')\n",
    "ft[\"spany\"]\n",
    "ft.get_subwords(\"pany\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenates a zero vector at location 0 for padding\n",
    "def padded_weights(weights):\n",
    "    \n",
    "    zero = torch.zeros([1, 100], dtype=torch.int32)\n",
    "    \n",
    "    weights = torch.cat((zero, weights), 0)\n",
    "    \n",
    "    return weights, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns the Embedding layer with pre-trained weights for word vectors\n",
    "def embedding_layer(weights):\n",
    "    \n",
    "    embedding = nn.Embedding.from_pretrained(weights, freeze=True, padding_idx=0)\n",
    "    \n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0152,  0.0435, -0.0073, -0.0884,  0.0664, -0.0027, -0.0515, -0.1511,\n",
       "          0.0180, -0.1195],\n",
       "        [ 0.0248,  0.1206, -0.0402,  0.0265,  0.0137, -0.1065,  0.0007, -0.1325,\n",
       "          0.0063, -0.0888],\n",
       "        [ 0.0185,  0.0195, -0.1128,  0.0521,  0.0658, -0.0568,  0.0518, -0.0650,\n",
       "         -0.1146,  0.0599],\n",
       "        [ 0.0449, -0.1273,  0.0169,  0.0863,  0.0446,  0.0424, -0.0136,  0.1266,\n",
       "         -0.0532,  0.0186],\n",
       "        [-0.0458, -0.0063,  0.0399,  0.0297, -0.0477, -0.0396,  0.0121, -0.0358,\n",
       "          0.0650, -0.0146],\n",
       "        [ 0.0074,  0.0594, -0.0624,  0.0389,  0.0395,  0.0046,  0.0941,  0.0610,\n",
       "          0.1056, -0.0180],\n",
       "        [ 0.0169,  0.0095,  0.0334,  0.0096,  0.0335,  0.0078,  0.0240,  0.0289,\n",
       "         -0.0027,  0.0461],\n",
       "        [ 0.0061, -0.0039, -0.0721,  0.0016, -0.0369, -0.0094, -0.0216, -0.0137,\n",
       "         -0.0080, -0.0882],\n",
       "        [ 0.0066,  0.0192,  0.0198,  0.0082,  0.0457, -0.0351,  0.0222,  0.0817,\n",
       "          0.0192,  0.0561],\n",
       "        [ 0.0330,  0.0249, -0.0507,  0.0583,  0.0415, -0.0243,  0.0019,  0.0975,\n",
       "          0.0226,  0.0605]])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = np.zeros((100,1))\n",
    "vec = np.array([ft[\"merhaba\"]]).T\n",
    "vector = torch.tensor(ft['merhaba'])\n",
    "vector = vector.reshape((10,10))\n",
    "vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Given the input string and the fasttext object, returns the vectorized representations obtained\n",
    "#from the embeddings of the fasttext model\n",
    "#words: a batch of strings that are the inputs to the model\n",
    "#padding: the max number of words per input. if num_words is less than padding, fill with zero vectors\n",
    "#in_vector: [batch_size, num_inp_words, (input_shape)] Tensor - reshape the vectors to \n",
    "#           (input_shape) matrices and stack them as channels\n",
    "def get_input_vectors(words, padding, ft, input_shape):\n",
    "    \n",
    "    tokens = []\n",
    "    batch_size = len(words)\n",
    "    \n",
    "    h, w = input_shape\n",
    "    \n",
    "    #loop over the batches to tokenize the inputs\n",
    "    for i in range(batch_size):\n",
    "        #Tokenize words using default fasttext tokenizer, which creates tokens \n",
    "        # by dividing splitting at word separating chars\n",
    "        tokens.append(fasttext.tokenize(words[i]))\n",
    "\n",
    "    #Create a matrix with batch_size batches, num token channels and 10x10 matrices to store the 100dim embeddings\n",
    "    in_vector = np.zeros((batch_size, padding, h, w))\n",
    "    \n",
    "    \n",
    "    #cycle over the tokens and get their vectors, reshape them to 10x10 and store in the corresponding \n",
    "    #channel in the return variable\n",
    "    for j in range(len(tokens)):\n",
    "        \n",
    "        i = 0\n",
    "\n",
    "        for token in tokens[j]:\n",
    "            \n",
    "            vector = torch.tensor(ft[token].astype(np.double))\n",
    "            #print(vector.shape)\n",
    "            vector = vector.reshape(h,w)\n",
    "            #print(vector.shape)\n",
    "#            print(vector.type())\n",
    "            in_vector[j][i] = vector\n",
    "\n",
    "            i=i+1\n",
    "            if(i == padding):\n",
    "                break\n",
    "\n",
    "    #create a tensor object to return\n",
    "    in_vector = torch.tensor(in_vector)\n",
    "\n",
    "    return in_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 25, 4])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_input_vectors([\"merhaba benim adim alicanhas\", \"olmaz oyle is\"], 4, ft, (25,4)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNet(Module):\n",
    "    def __init__(self):\n",
    "        super(CNet, self).__init__()\n",
    "        \n",
    "        #self.embedding_layer = Embedding.from_pretrained(weights, freeze=True, padding_idx=0)\n",
    "        \n",
    "        self.cnn_layers = Sequential(\n",
    "            #Convolutional Layer 1 [N, C, H, W]\n",
    "            #in: [N, 8, 10, 10]\n",
    "            #out: [N, 10, 9, 9]\n",
    "            Conv2d(4, 10, kernel_size=3, stride=1, padding=1),\n",
    "            BatchNorm2d(10),\n",
    "            ReLU(inplace=True),\n",
    "            MaxPool2d(kernel_size=2, stride=1),\n",
    "            \n",
    "            #Convolutional Layer 2\n",
    "            #in: [N, 10, 9, 9]\n",
    "            #out: [N, 30, 5, 5]\n",
    "            Conv2d(10, 30, kernel_size=5, stride=1),\n",
    "            BatchNorm2d(30),\n",
    "            ReLU(inplace=True),\n",
    "            MaxPool2d(kernel_size=2, stride=1),\n",
    "            \n",
    "            #Convolutional Layer 3\n",
    "            #in: [N, 30, 4, 4]\n",
    "            #out: [N, 50, 3, 3]\n",
    "            Conv2d(30, 50, kernel_size=1, stride=1),\n",
    "            BatchNorm2d(50),\n",
    "            ReLU(inplace=True),\n",
    "            MaxPool2d(kernel_size=2, stride=1),\n",
    "            \n",
    "            #Convolutional Layer 4\n",
    "            #in: [N, 50, 3, 3]\n",
    "            #out: [N, 100, 1, 1]\n",
    "            Conv2d(50, 100, kernel_size=3, stride=1),\n",
    "            BatchNorm2d(100),\n",
    "            ReLU(inplace=True),\n",
    "        )\n",
    "        \n",
    "        #The linear fully connected layer at the end of the network that outputs a classification\n",
    "        self.out_layers = Sequential(\n",
    "            Linear(100, 256),\n",
    "            ReLU(),\n",
    "            Linear(256, 512),\n",
    "            ReLU(),\n",
    "            Linear(512, 512),\n",
    "            ReLU(),\n",
    "            Linear(512, 256),\n",
    "            ReLU(),\n",
    "            Linear(256, 64),\n",
    "            ReLU(),\n",
    "            Linear(64, 16),\n",
    "            ReLU(),\n",
    "            Linear(16, 16),\n",
    "            ReLU(),\n",
    "            Linear(16, 1),\n",
    "            Sigmoid()\n",
    "        )\n",
    "        \n",
    "    #Define the forward pass through the model \n",
    "    def forward(self, x):\n",
    "        x = self.cnn_layers(x)\n",
    "        x = x.reshape((x.shape[0],x.shape[1]))\n",
    "        #print(\"x shape:\", x.shape)\n",
    "        x = self.out_layers(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 100, 1])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_input_vectors([\"merhaba benim adim alicanhas\", \"olmaz oyle is\"], 4, ft, (100,1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LNet(Module):\n",
    "    def __init__(self):\n",
    "        super(LNet, self).__init__()\n",
    "        self.drop_prob = 0.1\n",
    "        \n",
    "        #self.embedding_layer = Embedding.from_pretrained(weights, freeze=True, padding_idx=0)\n",
    "        \n",
    "        self.cnn_layers = Sequential(\n",
    "            #Convolutional Layer 1 [N, C, H, W]\n",
    "            #in: [N, 4, 100, 1]\n",
    "            #out: [N, 100, 100, 1]\n",
    "            Conv2d(4, 100, kernel_size=1, stride=1, padding=0),\n",
    "            BatchNorm2d(100),\n",
    "            ReLU(inplace=True),\n",
    "            \n",
    "            #Convolutional Layer 4\n",
    "            #in: [N, 100, 100, 1]\n",
    "            #out: [N, 1, 100, 1]\n",
    "            Conv2d(100, 1, kernel_size=1, stride=1),\n",
    "            nn.Dropout2d(self.drop_prob),\n",
    "            BatchNorm2d(1),\n",
    "            ReLU(inplace=True),\n",
    "        )\n",
    "        \n",
    "        #The linear fully connected layer at the end of the network that outputs a classification\n",
    "        self.out_layers = Sequential(\n",
    "            Linear(100, 256),\n",
    "            #Dropout(self.drop_prob),\n",
    "            BatchNorm1d(256),\n",
    "            ReLU(),\n",
    "            Linear(256, 512),\n",
    "            BatchNorm1d(512),\n",
    "            ReLU(),\n",
    "            Linear(512, 1024),\n",
    "            BatchNorm1d(1024),\n",
    "            ReLU(),\n",
    "            Linear(1024, 256),\n",
    "            BatchNorm1d(256),\n",
    "            ReLU(),\n",
    "            Linear(256, 256),\n",
    "            BatchNorm1d(256),\n",
    "            ReLU(),\n",
    "            Linear(256, 256),\n",
    "            BatchNorm1d(256),\n",
    "            ReLU(),\n",
    "            Linear(256, 256),\n",
    "            BatchNorm1d(256),\n",
    "            ReLU(),\n",
    "            Linear(256, 64),\n",
    "            BatchNorm1d(64),\n",
    "            ReLU(),\n",
    "            Linear(64, 64),\n",
    "            BatchNorm1d(64),\n",
    "            ReLU(),\n",
    "            Linear(64, 16),            \n",
    "            BatchNorm1d(16),\n",
    "            ReLU(),\n",
    "            Linear(16, 1),\n",
    "            Sigmoid()\n",
    "        )\n",
    "        \n",
    "    #Define the forward pass through the model \n",
    "    def forward(self, x):\n",
    "        x = self.cnn_layers(x)\n",
    "        #print(\"x shape:\", x.shape)\n",
    "        x = x.reshape((x.shape[0],x.shape[2]))\n",
    "        #print(\"x shape:\", x.shape)\n",
    "        x = self.out_layers(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, loss, optimizer):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    #X, Y = Variable(X_train), Variable(Y_train)\n",
    "\n",
    "    #if torch.cuda.is_available():\n",
    "    #    X = X.cuda()\n",
    "    #    Y = Y.cuda()\n",
    "    \n",
    "    last_loss = 0\n",
    "    \n",
    "    for i, data in enumerate(train_loader, 0):    \n",
    "\n",
    "        inputs, labels = data\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        X_out = model(inputs.float())\n",
    "\n",
    "        loss_tr = loss(X_out, labels)\n",
    "\n",
    "        loss_tr.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        #print(\"Epoch: \", e, \"\\t\", \"Batch: \", i, \"\\t\", \"Loss: \" + str(loss_tr) )\n",
    "        #s = torch.sum(model.out_layers[4].weight.data)\n",
    "        #print(s)\n",
    "        \n",
    "        if i == 23:\n",
    "            last_loss = loss_tr\n",
    "    \n",
    "    s = torch.sum(model.out_layers[7].weight.data)\n",
    "    \n",
    "    return last_loss, s\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, val_loader, loss):\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    test_loss = 0\n",
    "    \n",
    "    model = model.float()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in val_loader:\n",
    "            \n",
    "            sents, labels = data\n",
    "            \n",
    "            outputs = model(sents.float())\n",
    "            \n",
    "            test_loss = loss(outputs, labels)\n",
    "            \n",
    "            outputs = outputs>=0.5\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (outputs==labels).sum().item()\n",
    "    \n",
    "    accuracy = 100*correct/total\n",
    "\n",
    "    #print(\"Accuracy on the validation set of \", total, \" items is: \", (accuracy))\n",
    "    \n",
    "    return accuracy, test_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([250, 1])\n",
      "torch.Size([250, 4, 100, 1])\n",
      "Data_Val shape:  17 Data_tr:  233\n",
      "LNet(\n",
      "  (cnn_layers): Sequential(\n",
      "    (0): Conv2d(4, 100, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1): BatchNorm2d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(100, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (4): Dropout2d(p=0.1, inplace=False)\n",
      "    (5): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU(inplace=True)\n",
      "  )\n",
      "  (out_layers): Sequential(\n",
      "    (0): Linear(in_features=100, out_features=256, bias=True)\n",
      "    (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=512, out_features=1024, bias=True)\n",
      "    (7): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU()\n",
      "    (9): Linear(in_features=1024, out_features=256, bias=True)\n",
      "    (10): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (13): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (14): ReLU()\n",
      "    (15): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (16): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (19): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (20): ReLU()\n",
      "    (21): Linear(in_features=256, out_features=64, bias=True)\n",
      "    (22): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (23): ReLU()\n",
      "    (24): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (25): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (26): ReLU()\n",
      "    (27): Linear(in_features=64, out_features=16, bias=True)\n",
      "    (28): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (29): ReLU()\n",
      "    (30): Linear(in_features=16, out_features=1, bias=True)\n",
      "    (31): Sigmoid()\n",
      "  )\n",
      ")\n",
      "Total Parameters:  1170916\n",
      "Epoch:  0  Train Loss: 0.303  Train Weights: 1023.5  Test Loss: 0.240  Test Acc: 64.71\n",
      "Epoch:  1  Train Loss: 0.257  Train Weights: 1023.4  Test Loss: 0.248  Test Acc: 64.71\n",
      "Epoch:  2  Train Loss: 0.217  Train Weights: 1023.3  Test Loss: 0.217  Test Acc: 76.47\n",
      "Epoch:  3  Train Loss: 0.319  Train Weights: 1023.5  Test Loss: 0.243  Test Acc: 52.94\n",
      "Epoch:  4  Train Loss: 0.253  Train Weights: 1023.5  Test Loss: 0.261  Test Acc: 52.94\n",
      "Epoch:  5  Train Loss: 0.293  Train Weights: 1022.7  Test Loss: 0.255  Test Acc: 47.06\n",
      "Epoch:  6  Train Loss: 0.302  Train Weights: 1020.1  Test Loss: 0.250  Test Acc: 58.82\n",
      "Epoch:  7  Train Loss: 0.287  Train Weights: 1019.3  Test Loss: 0.265  Test Acc: 52.94\n",
      "Epoch:  8  Train Loss: 0.272  Train Weights: 1019.0  Test Loss: 0.249  Test Acc: 52.94\n",
      "Epoch:  9  Train Loss: 0.227  Train Weights: 1019.5  Test Loss: 0.228  Test Acc: 64.71\n",
      "Epoch:  10  Train Loss: 0.199  Train Weights: 1019.6  Test Loss: 0.227  Test Acc: 52.94\n",
      "Epoch:  11  Train Loss: 0.325  Train Weights: 1019.1  Test Loss: 0.232  Test Acc: 64.71\n",
      "Epoch:  12  Train Loss: 0.236  Train Weights: 1018.5  Test Loss: 0.240  Test Acc: 52.94\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X, Y, n = read_file_to_str(\"train.txt\")\n",
    "\n",
    "X = get_input_vectors(X, 4, ft, (100,1))\n",
    "Y = torch.tensor(Y)\n",
    "Y = Y.reshape(Y.shape[0], 1)\n",
    "print(Y.shape)\n",
    "print(X.shape)\n",
    "\n",
    "data = TensorDataset(X, Y)\n",
    "\n",
    "val_perc = 7/100\n",
    "n_val = int(n*val_perc)\n",
    "n_tr = n - n_val\n",
    "\n",
    "data_val, data_tr = random_split(data, [n_val, n_tr], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "train_loader = DataLoader(data_tr, batch_size=10, shuffle = True)\n",
    "val_loader = DataLoader(data_val, batch_size=n_val, shuffle=True)\n",
    "\n",
    "print(\"Data_Val shape: \", n_val, \"Data_tr: \", n_tr)\n",
    "\n",
    "#define the model\n",
    "model = LNet()\n",
    "#model = CNet()\n",
    "\n",
    "#optimizer\n",
    "optimizer = Adam(model.parameters(), lr = 0.01)\n",
    "\n",
    "#loss function\n",
    "loss = nn.MSELoss()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    loss = loss.cuda()\n",
    "    \n",
    "\n",
    "print(model)\n",
    "print(\"Total Parameters: \", count_parameters(model))\n",
    "\n",
    "model = model.float()\n",
    "\n",
    "train_losses = []\n",
    "train_weights = []\n",
    "test_accs = []\n",
    "test_losses = []\n",
    "\n",
    "\n",
    "epoch = 25\n",
    "\n",
    "for e in range(epoch):\n",
    "\n",
    "    train_loss, weight = train(train_loader, model, loss, optimizer)\n",
    "    test_acc, test_loss = test(model, val_loader, loss)\n",
    "    print(\"Epoch: \", e, \" Train Loss: %.3f\" %train_loss.item(), \" Train Weights: %.1f\" %weight.item(), \n",
    "          \" Test Loss: %.3f\" %test_loss, \" Test Acc: %.2f\" %test_acc,)\n",
    "    train_losses.append(train_loss)\n",
    "    test_accs.append(test_acc)\n",
    "    test_losses.append(test_loss)\n",
    "    train_weights.append(weight)\n",
    "\n",
    "plt.plot(train_losses, label=\"Training Losses\")\n",
    "plt.plot(test_losses, label=\"Test Losses\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file_to_str(file_path):\n",
    "    \n",
    "    file = open(file_path, 'r')\n",
    "    \n",
    "    data_x = []\n",
    "    data_y = []\n",
    "    \n",
    "    n = 0\n",
    "    \n",
    "    #Loop through the lines, and split at the comma for the inputs x and labels y\n",
    "    for line in file:\n",
    "        #print(line)\n",
    "        temp = line.split(\",\")\n",
    "        #print(temp)\n",
    "        #train.append({'x':temp[1] , 'y': float(temp[0])})\n",
    "        data_y.append(float(temp[0]))\n",
    "        data_x.append(temp[1])\n",
    "        n = n + 1\n",
    "\n",
    "    file.close()\n",
    "    \n",
    "    return data_x, data_y, n\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Train Examples:  250\n"
     ]
    }
   ],
   "source": [
    "_, _, n = read_file_to_str(\"train.txt\")\n",
    "print('Num Train Examples: ', n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
